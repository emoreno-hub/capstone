{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas  as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h3 as h3\n",
    "\n",
    "import re\n",
    "\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "from pathlib import Path\n",
    "from os import listdir\n",
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "#from commons import download_data, find_vcs_root\n",
    "\n",
    "path =  Path(os.getcwd())\n",
    "root = path.parent.absolute()\n",
    "\n",
    "# import libraries needed for upload / download to AWS\n",
    "import boto3\n",
    "import awswrangler\n",
    "from fiona.session import AWSSession\n",
    "import fiona\n",
    "# set name of S3 bucket\n",
    "s3_bucket = 'traffic-data-bucket'\n",
    "\n",
    "root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Base Table\n",
    "##### LA County shape file transposed to Uber Hexegons at level 8. ~.75 square km\n",
    "##### This process takes a shape file and maps it to hex files for a given level. The output of the mapping is the a unique hex_id for the hexegon and the shape geometry\n",
    "##### https://h3geo.org/docs/core-library/restable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aws_secrets import aws_access_key_id, aws_secret_access_key, aws_session_token\n",
    "\n",
    "my_session = boto3.Session(\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    aws_session_token = aws_session_token\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fiona.Env(session=AWSSession(my_session)):\n",
    "    gdf_all = gpd.read_file(f\"s3://{s3_bucket}/h3_processed_data/base_map_hex_all/base_map_hex_all.shp\")\n",
    "\n",
    "# gdf_all = gpd.read_file(root / 'X.data' / 'h3_processed_data' / 'base_map_hex_all' /'base_map_hex_all.shp')\n",
    "print(gdf_all.shape)\n",
    "gdf_all.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_all = gdf_all[~(gdf_all.hex_id == '0')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 City and District shape files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_label = awswrangler.s3.read_csv(path=f's3://{s3_bucket}/h3_processed_data/city_labels_hex.csv', boto3_session=my_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "district_labels = awswrangler.s3.read_csv(path=f's3://{s3_bucket}/h3_processed_data/district_labels_hex.csv', boto3_session=my_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Nodes\n",
    "##### LA County nodes - pulled from Ptyhon OSMNX. All street intersections\n",
    "#####   The lat and lon for each node was mapped hex id for joining onto the the county hex file\n",
    "##### https://github.com/gboeing/osmnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highway_cnts = awswrangler.s3.read_csv(path=f's3://{s3_bucket}/nodes_and_edges/nodes_highway_cnts.csv', boto3_session=my_session)\n",
    "\n",
    "\n",
    "display(highway_cnts.sample())\n",
    "highway_cnts.highway.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "street_cnts = awswrangler.s3.read_csv(path=f's3://{s3_bucket}/nodes_and_edges/nodes_street_count_cnts.csv', boto3_session=my_session)\n",
    "\n",
    "#display(street_cnts.sample())\n",
    "street_cnts_grps = street_cnts.groupby('hex_id').street_count.agg('max')\n",
    "street_cnts_grps = street_cnts_grps.reset_index()\n",
    "street_cnts_grps = street_cnts_grps[~(street_cnts_grps.hex_id == '0')]\n",
    "street_cnts_grps.columns = ['hex_id', 'node_street_count']\n",
    "street_cnts_grps.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_all = gdf_all.merge(street_cnts_grps, on = 'hex_id', how = 'left')\n",
    "gdf_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Edges\n",
    "#### LA County edges (streets) - pulled from Ptyhon OSMNX.\n",
    "##### These are the line geometry shape files. The will be joined using geo panda sjoin to the shape file for the hex\n",
    "##### https://github.com/gboeing/osmnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fiona.Env(session=AWSSession(my_session)):\n",
    "    edges = gpd.read_file(f\"s3://{s3_bucket}/nodes_and_edges/la_county_edges/la_county_edges.shp\")\n",
    "\n",
    "\n",
    "# edges = gpd.read_file(root / 'X.data' /  'nodes_and_edges' / 'la_county_edges' / 'la_county_edges.shp')\n",
    "print(edges.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Collision data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collision_hex = awswrangler.s3.read_csv(path=f's3://{s3_bucket}/h3_processed_data/collisions_hex.csv', boto3_session=my_session)\n",
    "\n",
    "\n",
    "collision_hex.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Nodes (intersections) \n",
    "\n",
    "### 2.1 Prep node files by making a wide table.  One unique row per hex id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highway_pivot = highway_cnts.pivot(index=\"hex_id\", columns=\"highway\", values=\"count\").fillna(0)\n",
    "highway_pivot.columns = 'node_'+highway_pivot.columns\n",
    "highway_pivot.reset_index(inplace = True)\n",
    "highway_pivot.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highway_pivot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orign_row_count = gdf_all.shape[0]\n",
    "gdf_all = gdf_all.merge(highway_pivot, on = 'hex_id', how = 'left')\n",
    "updated_row_count = gdf_all.shape[0]\n",
    "orign_row_count == updated_row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#improvment could be to create this list dynamically\n",
    "counts_col_list = ['node_street_count','node_crossing', 'node_give_way',\n",
    "       'node_milestone', 'node_mini_roundabout', 'node_motorway_junction',\n",
    "       'node_stop', 'node_traffic_signals', 'node_trailhead',\n",
    "       'node_turning_circle', 'node_turning_loop']\n",
    "\n",
    "gdf_all.update(gdf_all[counts_col_list].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_all.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Attach the neighboring nodes hex ids to the general table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h3 k_ring returns the ring of hexegons touching a given h3.  \n",
    "# Set level \n",
    "# skin = 1 is first ring plus the hex itself.  \n",
    "# skin = 2 is second ring out plus ring 1 plus the hex itself, ect...\n",
    "def rking_neighbors(row, skins):\n",
    "    neighbors = h3.k_ring(row.hex_id, skins)\n",
    "    neighbors_list = list(neighbors)\n",
    "    return(neighbors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_all['hex_neighbors_0_ids'] = gdf_all.apply(lambda x: rking_neighbors(x, skins = 0), axis=1)\n",
    "gdf_all['hex_neighbors_1_ids'] = gdf_all.apply(lambda x: rking_neighbors(x, skins = 1), axis=1)\n",
    "gdf_all['hex_neighbors_2_ids'] = gdf_all.apply(lambda x: rking_neighbors(x, skins = 2), axis=1)\n",
    "gdf_all[['hex_id', 'hex_neighbors_0_ids', 'hex_neighbors_1_ids', 'hex_neighbors_2_ids']].sample(2)\n",
    "\n",
    "#hex_id should be the same as hex_neighbors_0_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Neighboring Hex Counts\n",
    "#### For all the nodes columns, attach the count for the hex and it ring 1 and 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a table with a row for each hex neighbor 1\n",
    "gd_all_1_tall = gdf_all[['hex_id', 'hex_neighbors_1_ids']].explode('hex_neighbors_1_ids')\n",
    "#merge all the counts for each neibhbor hex id\n",
    "gd_all_1_tall = gd_all_1_tall.merge(gdf_all[counts_col_list + ['hex_id']], \n",
    "                                    left_on = 'hex_neighbors_1_ids', \n",
    "                                    right_on = 'hex_id',\n",
    "                                    how = 'inner')\n",
    "#print(gd_all_1_tall.columns)\n",
    "gd_all_1_tall = gd_all_1_tall[['hex_id_x'] + counts_col_list]\n",
    "gd_all_1_grp_sum = gd_all_1_tall.groupby('hex_id_x')[counts_col_list].agg('sum')\n",
    "gd_all_1_grp_cnt = gd_all_1_tall.groupby('hex_id_x')[counts_col_list[0]].agg('count')\n",
    "#gd_all_1_grp.columns = ['hex_id', 'neighbor_1_collision_count']\n",
    "gd_all_1_grp_sum.columns = 'neighbor_1_' + gd_all_1_grp_sum.columns\n",
    "\n",
    "#sum over the hex id neighbors\n",
    "gd_all_1_grp_sum.index.names = ['hex_id']\n",
    "gd_all_1_grp_sum.reset_index(inplace = True)\n",
    "gd_all_1_grp_cnt = gd_all_1_grp_cnt.reset_index()\n",
    "gd_all_1_grp_cnt.columns = ['hex_id', 'neighbor_1_count']\n",
    "\n",
    "gdf_all = gdf_all.merge(gd_all_1_grp_sum, on = 'hex_id', how = 'left')\n",
    "gdf_all = gdf_all.merge(gd_all_1_grp_cnt, on = 'hex_id', how = 'left')\n",
    "gdf_all.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat the cell above for neighbor id 2\n",
    "gd_all_2_tall = gdf_all[['hex_id', 'hex_neighbors_2_ids']].explode('hex_neighbors_2_ids')\n",
    "gd_all_2_tall = gd_all_2_tall.merge(gdf_all[counts_col_list + ['hex_id']], \n",
    "                                    left_on = 'hex_neighbors_2_ids', \n",
    "                                    right_on = 'hex_id',\n",
    "                                    how = 'inner')\n",
    "#print(gd_all_1_tall.columns)\n",
    "gd_all_2_tall = gd_all_2_tall[['hex_id_x'] + counts_col_list]\n",
    "gd_all_2_grp_sum = gd_all_2_tall.groupby('hex_id_x')[counts_col_list].agg('sum')\n",
    "gd_all_2_grp_cnt = gd_all_2_tall.groupby('hex_id_x')[counts_col_list[0]].agg('count')\n",
    "#gd_all_1_grp.columns = ['hex_id', 'neighbor_1_collision_count']\n",
    "gd_all_2_grp_sum.columns = 'neighbor_2_' + gd_all_2_grp_sum.columns\n",
    "\n",
    "gd_all_2_grp_sum.index.names = ['hex_id']\n",
    "gd_all_2_grp_sum.reset_index(inplace = True)\n",
    "gd_all_2_grp_cnt = gd_all_2_grp_cnt.reset_index()\n",
    "gd_all_2_grp_cnt.columns = ['hex_id', 'neighbor_2_count']\n",
    "\n",
    "gdf_all = gdf_all.merge(gd_all_2_grp_sum, on = 'hex_id', how = 'left')\n",
    "gdf_all = gdf_all.merge(gd_all_2_grp_cnt, on = 'hex_id', how = 'left')\n",
    "gdf_all.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to make sure all joins didn't add rows\n",
    "print(gdf_all.shape[0])\n",
    "updated_row_count = gdf_all.shape[0]\n",
    "orign_row_count == updated_row_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Nearest Hex Neighbor Average\n",
    "Some hex do not boarder other streets. Coast or just in the middle of mountains.  Taking average of the hex with streets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all columns with the following pattern\n",
    "r = re.compile(\"neighbor_1_*\")\n",
    "neighbor_col_list = list(filter(r.match, gdf_all.columns))\n",
    "neighbor_col_list.remove('neighbor_1_count')\n",
    "\n",
    "\n",
    "for nc in neighbor_col_list:\n",
    "    gdf_all[nc + '_ave'] = gdf_all[nc] / gdf_all.neighbor_1_count\n",
    "\n",
    "    #get all columns with the following pattern\n",
    "r = re.compile(\"neighbor_2_*\")\n",
    "neighbor_col_list = list(filter(r.match, gdf_all.columns)) \n",
    "neighbor_col_list.remove('neighbor_2_count')\n",
    "\n",
    "for nc in neighbor_col_list:\n",
    "    gdf_all[nc + '_ave'] = gdf_all[nc] / gdf_all.neighbor_2_count\n",
    "\n",
    "gdf_all.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. City and Distric Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_all = gdf_all.merge(city_label, on = 'hex_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_label.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_all = gdf_all.merge(district_labels, on = 'hex_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to make sure all joins didn't add rows\n",
    "print(gdf_all.shape[0])\n",
    "updated_row_count = gdf_all.shape[0]\n",
    "orign_row_count == updated_row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_all.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_all.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Edges (streets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depending on the version of geopandas installed, 'op' and 'predicate' arguments are the same\n",
    "gdf_all_edge_exploded = gpd.sjoin(gdf_all[['hex_id', 'geometry']], edges.to_crs(epsg=3857), how='inner', op='intersects')\n",
    "\n",
    "# gdf_all_edge_exploded = gpd.sjoin(gdf_all[['hex_id', 'geometry']], edges.to_crs(epsg=3857), how='inner', predicate='intersects')\n",
    "gdf_all_edge_exploded.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_all_edge_exploded.highway.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_all_edge_exploded.bridge.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_all_edge_exploded.tunnel.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Create road type indicator (highway)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_all_edge_exploded['motorway_id'] = gdf_all_edge_exploded['highway'].isin(['motorway'])\n",
    "gdf_all_edge_exploded['motorway_link_id'] = gdf_all_edge_exploded['highway'].isin(['motorway_link'])\n",
    "gdf_all_edge_exploded['living_street_id'] = gdf_all_edge_exploded['highway'].isin(['living_street'])\n",
    "gdf_all_edge_exploded['bridge_id'] = gdf_all_edge_exploded['bridge'].isin(['yes'])\n",
    "gdf_all_edge_exploded['tunnel_id'] = gdf_all_edge_exploded['tunnel'].isin(['yes'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_grp_speed = gdf_all_edge_exploded.groupby('hex_id')[['speed_kph']].agg(['max','min']).reset_index()\n",
    "edge_grp_speed.columns = ['hex_id', 'edge_speed_kph_max', 'edge_speek_kph_min']\n",
    "edge_grp_speed.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_grp_lanes = gdf_all_edge_exploded.groupby('hex_id')[['lanes']].agg(['max','min']).reset_index()\n",
    "edge_grp_lanes.columns = ['hex_id', 'edge_lanes_max', 'edge_lanes_min']\n",
    "edge_grp_lanes.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_grp_motorway_id = gdf_all_edge_exploded.groupby('hex_id')[['motorway_id']].agg(['max']).reset_index()\n",
    "edge_grp_motorway_id.columns = ['hex_id', 'edge_motorway_id']\n",
    "edge_grp_motorway_id.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_grp_motorway_link_id = gdf_all_edge_exploded.groupby('hex_id')[['motorway_link_id']].agg(['max']).reset_index()\n",
    "edge_grp_motorway_link_id.columns = ['hex_id', 'edge_motorway_link_id']\n",
    "edge_grp_motorway_link_id.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_grp_living_street_id = gdf_all_edge_exploded.groupby('hex_id')[['living_street_id']].agg(['max']).reset_index()\n",
    "edge_grp_living_street_id.columns = ['hex_id', 'edge_living_street_id']\n",
    "edge_grp_living_street_id.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_grp_bridge_id = gdf_all_edge_exploded.groupby('hex_id')[['bridge_id']].agg(['max']).reset_index()\n",
    "edge_grp_bridge_id.columns = ['hex_id', 'edge_bridge_id']\n",
    "edge_grp_bridge_id.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_grp_oneway_id = gdf_all_edge_exploded.groupby('hex_id')[['oneway']].agg(['max']).reset_index()\n",
    "edge_grp_oneway_id.columns = ['hex_id', 'edge_oneway_id']\n",
    "edge_grp_oneway_id.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_grp_tunnel_id = gdf_all_edge_exploded.groupby('hex_id')[['tunnel_id']].agg(['max']).reset_index()\n",
    "edge_grp_tunnel_id.columns = ['hex_id', 'edge_tunnel_id']\n",
    "edge_grp_tunnel_id.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_all = gdf_all.merge(edge_grp_speed, how = 'left', on = 'hex_id')\n",
    "print(gdf_all.shape[0])\n",
    "updated_row_count = gdf_all.shape[0]\n",
    "orign_row_count == updated_row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_all = gdf_all.merge(edge_grp_lanes, how = 'left', on = 'hex_id')\n",
    "print(gdf_all.shape[0])\n",
    "updated_row_count = gdf_all.shape[0]\n",
    "orign_row_count == updated_row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_all = gdf_all.merge(edge_grp_motorway_id, how = 'left', on = 'hex_id')\n",
    "print(gdf_all.shape[0])\n",
    "updated_row_count = gdf_all.shape[0]\n",
    "orign_row_count == updated_row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_all = gdf_all.merge(edge_grp_motorway_link_id, how = 'left', on = 'hex_id')\n",
    "print(gdf_all.shape[0])\n",
    "updated_row_count = gdf_all.shape[0]\n",
    "orign_row_count == updated_row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_all = gdf_all.merge(edge_grp_living_street_id, how = 'left', on = 'hex_id')\n",
    "print(gdf_all.shape[0])\n",
    "updated_row_count = gdf_all.shape[0]\n",
    "orign_row_count == updated_row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_all = gdf_all.merge(edge_grp_bridge_id, how = 'left', on = 'hex_id')\n",
    "print(gdf_all.shape[0])\n",
    "updated_row_count = gdf_all.shape[0]\n",
    "orign_row_count == updated_row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_all = gdf_all.merge(edge_grp_oneway_id, how = 'left', on = 'hex_id')\n",
    "print(gdf_all.shape[0])\n",
    "updated_row_count = gdf_all.shape[0]\n",
    "orign_row_count == updated_row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_all = gdf_all.merge(edge_grp_tunnel_id, how = 'left', on = 'hex_id')\n",
    "print(gdf_all.shape[0])\n",
    "updated_row_count = gdf_all.shape[0]\n",
    "orign_row_count == updated_row_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Create a filter hex file to only include hex's with a intersection or street (node or an edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a list of all hex that have an edge or node\n",
    "gdf_edge_filter = gpd.sjoin(gdf_all, edges.to_crs(epsg=3857), how = 'inner')\n",
    "gdf_edge_filter.reset_index(inplace = True)\n",
    "\n",
    "gdf_edge_filter = gdf_edge_filter[['hex_id']].drop_duplicates()\n",
    "gdf_node_filter = highway_cnts[['hex_id']].drop_duplicates()\n",
    "\n",
    "#row bind the two pandas\n",
    "gdf_filtered = pd.concat([gdf_edge_filter, gdf_node_filter], axis = 0)\n",
    "\n",
    "#create a vector of valid hex ids\n",
    "valid_array = gdf_filtered['hex_id'].values\n",
    "#print(valid_array)\n",
    "print(gdf_all.shape)\n",
    "\n",
    "#\n",
    "mask = gdf_all['hex_id'].isin(valid_array)\n",
    "\n",
    "gdf_all['valid_accident_location_filter'] = mask\n",
    "\n",
    "#gdf_all = gdf_all[gdf_all['valid_accident_location_filter'] == True]\n",
    "\n",
    "#gdf_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Collisions by Year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Neighbor collisions count.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collision_hex.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collision_hex_grp = collision_hex.groupby(['hex_id', 'collision_year']).accident_count.agg('sum').to_frame('collisions').reset_index()\n",
    "collision_hex_grp['collision_year'] = collision_hex_grp['collision_year']\n",
    "collision_hex_grp.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_all_0_tall = gdf_all[['hex_id', 'hex_neighbors_0_ids']].explode('hex_neighbors_0_ids')\n",
    "gd_all_0_tall = gd_all_0_tall.merge(collision_hex_grp, \n",
    "                                    left_on = 'hex_neighbors_0_ids', \n",
    "                                    right_on = 'hex_id',\n",
    "                                    how = 'inner')\n",
    "\n",
    "gd_all_0_tall = gd_all_0_tall[['hex_id_x', 'collision_year', 'collisions']]\n",
    "\n",
    "gd_all_0_tall = gd_all_0_tall.groupby(['hex_id_x', 'collision_year']).collisions.agg('sum').to_frame('neighbor_0_collision').reset_index()\n",
    "#display(gd_all_1_tall.head(1))\n",
    "\n",
    "pivot_neighbor_0 = gd_all_0_tall.pivot_table(index = 'hex_id_x', columns = 'collision_year', values = 'neighbor_0_collision')\n",
    "#display(pivot_neighbor_1.head(1))\n",
    "#print(pivot_neighbor_1.columns)\n",
    "pivot_neighbor_0.columns = [\"_\".join(('collisions_neighbor_0',str(j))) for j in pivot_neighbor_0.columns]\n",
    "pivot_neighbor_0.index.names = ['hex_id']\n",
    "pivot_neighbor_0 = pivot_neighbor_0.reset_index()\n",
    "pivot_neighbor_0 = pivot_neighbor_0.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_all_1_tall = gdf_all[['hex_id', 'hex_neighbors_1_ids']].explode('hex_neighbors_1_ids')\n",
    "gd_all_1_tall = gd_all_1_tall.merge(collision_hex_grp, \n",
    "                                    left_on = 'hex_neighbors_1_ids', \n",
    "                                    right_on = 'hex_id',\n",
    "                                    how = 'inner')\n",
    "\n",
    "gd_all_1_tall = gd_all_1_tall[['hex_id_x', 'collision_year', 'collisions']]\n",
    "\n",
    "gd_all_1_tall = gd_all_1_tall.groupby(['hex_id_x', 'collision_year']).collisions.agg('sum').to_frame('neighbor_1_collision').reset_index()\n",
    "#display(gd_all_1_tall.head(1))\n",
    "\n",
    "pivot_neighbor_1 = gd_all_1_tall.pivot_table(index = 'hex_id_x', columns = 'collision_year', values = 'neighbor_1_collision')\n",
    "#display(pivot_neighbor_1.head(1))\n",
    "#print(pivot_neighbor_1.columns)\n",
    "pivot_neighbor_1.columns = [\"_\".join(('collisions_neighbor_1',str(j))) for j in pivot_neighbor_1.columns]\n",
    "pivot_neighbor_1.index.names = ['hex_id']\n",
    "pivot_neighbor_1 = pivot_neighbor_1.reset_index()\n",
    "pivot_neighbor_1 = pivot_neighbor_1.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_all_2_tall = gdf_all[['hex_id', 'hex_neighbors_2_ids']].explode('hex_neighbors_2_ids')\n",
    "gd_all_2_tall = gd_all_2_tall.merge(collision_hex_grp, \n",
    "                                    left_on = 'hex_neighbors_2_ids', \n",
    "                                    right_on = 'hex_id',\n",
    "                                    how = 'inner')\n",
    "\n",
    "gd_all_2_tall = gd_all_2_tall[['hex_id_x', 'collision_year', 'collisions']]\n",
    "\n",
    "gd_all_2_tall = gd_all_2_tall.groupby(['hex_id_x', 'collision_year']).collisions.agg('sum').to_frame('neighbor_1_collision').reset_index()\n",
    "#display(gd_all_1_tall.head(1))\n",
    "\n",
    "pivot_neighbor_2 = gd_all_2_tall.pivot_table(index = 'hex_id_x', columns = 'collision_year', values = 'neighbor_1_collision')\n",
    "#display(pivot_neighbor_1.head(1))\n",
    "#print(pivot_neighbor_1.columns)\n",
    "pivot_neighbor_2.columns = [\"_\".join(('collisions_neighbor_2',str(j))) for j in pivot_neighbor_2.columns]\n",
    "pivot_neighbor_2.index.names = ['hex_id']\n",
    "pivot_neighbor_2 = pivot_neighbor_2.reset_index()\n",
    "pivot_neighbor_2 = pivot_neighbor_2.fillna(0)\n",
    "pivot_neighbor_2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pivot_neighbor_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_all.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_all = gdf_all.merge(pivot_neighbor_0, on = 'hex_id', how = 'left')\n",
    "gdf_all = gdf_all.merge(pivot_neighbor_1, on = 'hex_id', how = 'left')\n",
    "gdf_all = gdf_all.merge(pivot_neighbor_2, on = 'hex_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_all.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_all.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to CSV and upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awswrangler.s3.to_csv(df=gdf_all, path = f's3://{s3_bucket}/joined_data/base_location_data.csv', index=False,\n",
    "                       boto3_session=my_session, use_threads=True\n",
    "                       )\n",
    "\n",
    "# gdf_all.to_csv(root / 'X.data' / 'joined_data' / 'base_location_data.csv', index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
