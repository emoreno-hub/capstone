{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d93753c-f75a-4105-8400-c33909b53fd3",
   "metadata": {},
   "source": [
    "# Generate a Hexgrid Geodataframe for Los Angeles County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ca6fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas  as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import libpysal\n",
    "from tobler.util import h3fy\n",
    "from h3 import h3\n",
    "#import descartes\n",
    "\n",
    "\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "from pathlib import Path\n",
    "from os import listdir\n",
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "path =  Path(os.getcwd())\n",
    "root = path.parent.absolute()\n",
    "\n",
    "# import libraries needed for upload / download to AWS\n",
    "import boto3\n",
    "import awswrangler\n",
    "from fiona.session import AWSSession\n",
    "import fiona\n",
    "# set name of s3 bucket\n",
    "s3_bucket = 'traffic-data-bucket'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52243c1d-e40e-4dd9-b8b3-1fddfb35c320",
   "metadata": {},
   "source": [
    "#### Troubleshooting AWS Wrangler\n",
    "If you cannot import AWS Wrangler due to the error `AttributeError: module 'multiprocessing' has no attribute 'connection'` then try downgrading some of the dependencies by using: `pip install fsspec==0.6.3 PyAthena==1.10.2 s3fs==0.4.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af2f895-7a81-4073-aaae-3c1717dd2012",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Create Boto3 session\n",
    "Start by creating a boto3 session so that we can connect to the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954901cc-6a02-47ae-8cf4-61510b16789e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aws_secrets import aws_access_key_id, aws_secret_access_key, aws_session_token\n",
    "\n",
    "my_session = boto3.Session(\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    aws_session_token = aws_session_token\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be0b9ed-4845-437c-b4b6-3a715dfaa370",
   "metadata": {},
   "source": [
    "## 2. Read shapefile of Los Angeles County from S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8f6ac8-b90c-456e-8477-60827725bb5a",
   "metadata": {},
   "source": [
    "Next, use fiona to read the shapefile into GeoPandas.  If you try and read the shapefile from the S3 bucket directly into GeoPandas without using Fiona it will issue a timeout error.  The link below explains in further detail.\n",
    "\n",
    "https://github.com/Toblerity/Fiona/issues/1055"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e1a8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fiona.Env(session=AWSSession(my_session)):\n",
    "    gdf = gpd.read_file(f\"s3://{s3_bucket}/raw_data/la_county_website_data/County_Boundaries/County_Boundaries.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6301d944-1103-4596-a1c3-f569d308d1d1",
   "metadata": {},
   "source": [
    "## 3. Create a hexgrid over Los Angeles County\n",
    "\n",
    "### what does `.eq(22)` mean???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da56daa-53c6-467b-a24d-daea66252094",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gdf.loc[gdf['OBJECTID'].eq(22)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26ff4b9-4e0f-4adc-ab7d-43497cfae013",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5098c1d4-f543-47e8-b502-579dc84aebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the hexagon level\n",
    "h3_level = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b9dee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_hex = h3fy(source=gdf, resolution=h3_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aac4754",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gdf_hex.columns)\n",
    "gdf_hex.reset_index(inplace = True)\n",
    "gdf_hex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d009599",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_hex.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28382827-ea4f-4ed3-9d60-dc63abea005a",
   "metadata": {},
   "source": [
    "## 4. Visualize hexgrids over Los Angeles County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185f86e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextily as ctx\n",
    "ax = gdf_hex.plot(markersize=0.01, alpha = .3, edgecolors= \"black\", figsize=(20, 25))\n",
    "\n",
    "ctx.add_basemap(ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ae0a70-c950-4cc5-ad14-202926e23bcc",
   "metadata": {},
   "source": [
    "## 5. Create shapefile of hexgrids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46b99d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_hex[['hex_id', 'geometry']].to_file((root / 'X.data' / 'h3_processed_data' / 'base_map_hex_all' / 'base_map_hex_all.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b29e31-53b2-4f48-b3db-688466c02bd6",
   "metadata": {},
   "source": [
    "### 5.1 Create Boto3 session and upload shapefile data to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfc63c8-1536-4c96-82a6-ef9f7177d9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aws_secrets import aws_access_key_id, aws_secret_access_key, aws_session_token\n",
    "\n",
    "my_session = boto3.Session(\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    aws_session_token = aws_session_token\n",
    "\n",
    ")\n",
    "\n",
    "# create client session\n",
    "s3 = my_session.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5f8c2d-dbfb-40f3-8192-a6dae2e904a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "basemap_data_path = root / 'X.data' / 'h3_processed_data' / 'base_map_hex_all' \n",
    "\n",
    "for file in os.listdir(basemap_data_path):\n",
    "    upload_file_key = 'h3_processed_data/base_map_hex_all/' + str(file)\n",
    "    local_file_path = '../X.data/h3_processed_data/base_map_hex_all/' + str(file)\n",
    "    # upload to s3\n",
    "    s3.upload_file(Filename = local_file_path, Bucket = s3_bucket, Key = upload_file_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7414c4e5-ce82-476a-84fc-291a7f777799",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:capstone]",
   "language": "python",
   "name": "conda-env-capstone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
