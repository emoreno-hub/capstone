{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Test, Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "\n",
    "import boto3\n",
    "import awswrangler\n",
    "\n",
    "import re\n",
    "\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "from pathlib import Path\n",
    "from os import listdir\n",
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "#from commons import download_data, find_vcs_root\n",
    "\n",
    "path =  Path(os.getcwd())\n",
    "root = path.parent.absolute()\n",
    "\n",
    "root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Boto3 session\n",
    "Start by creating a boto3 session so that we can connect to the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aws_secrets import aws_access_key_id, aws_secret_access_key, aws_session_token\n",
    "\n",
    "my_session = boto3.Session(\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    aws_session_token = aws_session_token\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1  Base location data\n",
    "##### LA County shape file transposed to Uber Hexegons at level 8. ~.75 square km\n",
    "##### Import all hex and make a list of over day, hour and year and attach a random number for \n",
    "##### https://h3geo.org/docs/core-library/restable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_df = awswrangler.s3.read_csv(path='s3://traffic-data-bucket/joined_data/base_location_data.csv', boto3_session=my_session)\n",
    "\n",
    "\n",
    "# gdf_df = pd.read_csv(root / 'X.data' / 'joined_data' / 'base_location_data.csv')\n",
    "print(gdf_df.shape)\n",
    "valid_mask = gdf_df['valid_accident_location_filter'] == True\n",
    "gdf_valid_df = gdf_df[valid_mask]\n",
    "gdf_valid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_df = gdf_df[~(gdf_df.hex_id == '0')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Negative sample data\n",
    "Download negative sample dataframes generated by notebook `13.base_neg_sample_build.ipynb` and concatenate into one dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 bucket name and folder containing collision data\n",
    "raw_s3_bucket = 'traffic-data-bucket'\n",
    "raw_path_dir = 'neg_samples'\n",
    "\n",
    "# path of S3 bucket where collision data is stored\n",
    "raw_path = f\"s3://{raw_s3_bucket}/{raw_path_dir}\"\n",
    "\n",
    "# read data from S3 bucket\n",
    "neg_sample_df = awswrangler.s3.read_csv(path=raw_path, path_suffix=['.csv'], dataset=True,\n",
    "                                 boto3_session=my_session, use_threads=True, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collision_year_list = [2014, 2015, 2016, 2017, 2018, 2019, 2020]\n",
    "# neg_sample_dict = {}\n",
    "# for year in collision_year_list:\n",
    "#     neg_sample_dict[year] = pd.read_csv(root / 'X.data' / 'neg_samples' / ('neg_samples_' + str(year) + '.csv'),low_memory = False)\n",
    "# neg_sample_df = pd.concat(neg_sample_dict.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_sample_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_doy_to_date(row):\n",
    "    doy = str(row.doy)\n",
    "    year = str(row.year)\n",
    "    doy.rjust(3 + len(doy), '0')\n",
    "    new_date = datetime.strptime(year + \"-\" + doy, \"%Y-%j\").strftime(\"%m-%d-%Y\")\n",
    "    return new_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choices\n",
    "def sample_date_time_creation(frame):\n",
    "    frame['collision_date'] = pd.to_datetime(frame['date'])\n",
    "    frame['collision_month']  = frame['collision_date'].dt.month\n",
    "    frame['collision_dayofweek']  = frame['collision_date'].dt.dayofweek\n",
    "    frame['collision_year']  = frame['year']\n",
    "    frame['accident_count'] = 0\n",
    "    # panda frame hours range from 0 to 23\n",
    "    frame['collision_hour'] = choices(range(24),k=frame.shape[0])\n",
    "    frame = frame[['hex_id', 'collision_year', 'collision_month', 'collision_dayofweek', 'collision_hour', 'accident_count']]\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_sample_df['date'] = neg_sample_df.apply(convert_doy_to_date, axis=1)\n",
    "neg_sample_df = sample_date_time_creation(neg_sample_df)\n",
    "neg_sample_df['accident_count'] = 0\n",
    "neg_sample_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Positive sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sample_df = awswrangler.s3.read_csv(path='s3://traffic-data-bucket/h3_processed_data/collisions_hex.csv', boto3_session=my_session)\n",
    "\n",
    "# pos_sample_df = pd.read_csv(root / 'X.data' / 'h3_processed_data' / 'collisions_hex.csv', low_memory = False)\n",
    "pos_sample_df = pos_sample_df[['hex_id', 'collision_year', 'collision_month', 'collision_dayofweek', 'collision_hour', 'accident_count']]\n",
    "print(pos_sample_df.shape)\n",
    "pos_sample_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sample_df = pos_sample_df[pos_sample_df['hex_id'].isin(gdf_valid_df['hex_id'])]\n",
    "pos_sample_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate and attach test/train/validate and out of time.\n",
    "neg_pos_sample_df = pd.concat([pos_sample_df, neg_sample_df])\n",
    "neg_pos_sample_df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_pos_sample_df[neg_pos_sample_df['accident_count'] == 0].collision_year.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_pos_sample_df[neg_pos_sample_df['accident_count'] == 1].collision_year.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_pos_sample_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create train-test-validation split column\n",
    "This column is used for...???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random floating point values\n",
    "from random import seed\n",
    "from random import random\n",
    "# seed random number generator\n",
    "seed(1)\n",
    "random_list = list()\n",
    "# generate random numbers between 0-1\n",
    "for _ in range(neg_pos_sample_df.shape[0]):\n",
    "\trandom_list.append(random())\n",
    "len(random_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_pos_sample_df['random'] = pd.Series(random_list)\n",
    "neg_pos_sample_df['ttv_split'] = np.where(neg_pos_sample_df['random']<=.5, 'Train',\n",
    "                                 np.where(neg_pos_sample_df['random']<=.8, 'Test','Validate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_pos_sample_df.ttv_split.value_counts()/neg_pos_sample_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_pos_sample_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awswrangler.s3.to_csv(df=neg_pos_sample_df, path = 's3://traffic-data-bucket/TTV_splits/TTV_data.csv', index=False,\n",
    "                       boto3_session=my_session, use_threads=True)\n",
    "\n",
    "# neg_pos_sample_df.to_csv(root / 'X.data' / 'TTV_splits' / 'TTV_data.csv', index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:capstone]",
   "language": "python",
   "name": "conda-env-capstone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
